{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Aspect-based financial sentiment analysis\n",
    "\n",
    "Given a text instance in the financial domain (microblog message, news statement or headline) in English, detect the \n",
    "* target aspects which are mentioned in the text (from a pre-defined list of aspect classes) and,\n",
    "* predict the sentiment score for each of the mentioned targets. Sentiment scores will be defined using continuous numeric values ranged from -1(negative) to 1(positive). \n",
    "\n",
    "Systems will be evaluated with regard to aspect classification, sentiment classification and aspect-sentiment attachment. Participating systems will be evaluated with regard to precision, recall and F1-score for aspect classification approaches and regard to MSE and R Squared(R^2) metrics for sentiment prediction approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file =\"data/task1_post_ABSA_train.json\"\n",
    "test_file =\"data/task1_post_ABSA_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_data_from_json(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1]['info'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_dict(master_dict):\n",
    "    sentence = []\n",
    "    aspect = []\n",
    "    for key in master_dict.keys():\n",
    "        for info in master_dict[key]['info']:\n",
    "            sentence.append(eval(info['snippets'])[0])\n",
    "            aspect.append(eval(info['aspects'])[0])\n",
    "    return sentence,aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence,aspect = extract_data_from_dict(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'snippets': \"['Slowly adding some $FIO here but gotta be careful']\",\n",
       "  'sentiment_score': '0.459',\n",
       "  'target': 'FIO',\n",
       "  'aspects': \"['Stock/Price Action/Bullish/Bull Position']\"}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(train_file)\n",
    "df[14860]['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, train = True):\n",
    "    df = pd.read_json(filename)\n",
    "    \n",
    "    ids = []\n",
    "    aspects = []\n",
    "    snippets = []\n",
    "    companies = []\n",
    "    sentences = []\n",
    "    sentiment_scores = []\n",
    "    \n",
    "#     def getAspects(aspect):\n",
    "#         aspect = aspect.replace('[', '')\n",
    "#         aspect = aspect.replace(']', '')\n",
    "#         aspect = aspect.replace('\\'', '')\n",
    "#         return aspect.split('/')\n",
    "    aspect_pairs = {'corporate':{}, 'stock':{}, 'economy':{}, 'market':{}}\n",
    "    for id in df:\n",
    "        ids.append(id)\n",
    "        companies.append(df[id]['info'][0]['target'].lower())\n",
    "        sentences.append(df[id]['sentence'].lower())\n",
    "        snippets.append(df[id]['info'][0]['snippets'].lower())\n",
    "        \n",
    "        if (train):\n",
    "#             print(df[id]['info'][0]['aspects'].lower())\n",
    "            aspects.append((df[id]['info'][0]['aspects'].lower()).split(\"/\")[1].replace(']\"',\"\").replace(\"]\",\"\").replace('\"',\"\").replace(\"'\",\"\"))\n",
    "            sentiment_scores.append(float((df[id]['info'][0]['sentiment_score'])))\n",
    "                                    \n",
    "    if (train):\n",
    "        return sentences, snippets, companies, aspects, np.asarray(sentiment_scores)\n",
    "    return ids, sentences, snippets       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-67855618756b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-67855618756b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    train_aspects[7].split(\"/\")[]\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "train_aspects[7].split(\"/\")[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[8]['info'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(train_file)\n",
    "train_sentences, train_snippets, train_companies, train_aspects, train_sentiment_scores = load_data(train_file, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_aspects\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "X =mlb.fit_transform(X)\n",
    "# enc = OneHotEncoder()\n",
    "# enc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_l2_pair = {'Corporate': ['Company Communication',\n",
    "  'Dividend Policy',\n",
    "  'Regulatory',\n",
    "  'Reputation',\n",
    "  'Financial',\n",
    "  'Strategy',\n",
    "  'Technical Analysis',\n",
    "  'Rumors',\n",
    "  'Legal',\n",
    "  'Appointment',\n",
    "  'M&A',\n",
    "  'Risks',\n",
    "  'Sales'],\n",
    " 'Economy': ['Central Banks', 'Trade'],\n",
    " 'Market': ['Volatility', 'Market', 'Conditions', 'Currency'],\n",
    " 'Stock': ['Signal',\n",
    "  'Technical Analysis',\n",
    "  'Insider Activity',\n",
    "  'IPO',\n",
    "  'Coverage',\n",
    "  'Fundamentals',\n",
    "  'Options',\n",
    "  'Buyside',\n",
    "  'Price Action']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_l1_pair = {'Appointment': ['Corporate'],\n",
    " 'Buyside': ['Stock'],\n",
    " 'Central Banks': ['Economy'],\n",
    " 'Company Communication': ['Corporate'],\n",
    " 'Conditions': ['Market'],\n",
    " 'Coverage': ['Stock'],\n",
    " 'Currency': ['Market'],\n",
    " 'Dividend Policy': ['Corporate'],\n",
    " 'Financial': ['Corporate'],\n",
    " 'Fundamentals': ['Stock'],\n",
    " 'IPO': ['Stock'],\n",
    " 'Insider Activity': ['Stock'],\n",
    " 'Legal': ['Corporate'],\n",
    " 'M&A': ['Corporate'],\n",
    " 'Market': ['Market'],\n",
    " 'Options': ['Stock'],\n",
    " 'Price Action': ['Stock'],\n",
    " 'Regulatory': ['Corporate'],\n",
    " 'Reputation': ['Corporate'],\n",
    " 'Risks': ['Corporate'],\n",
    " 'Rumors': ['Corporate'],\n",
    " 'Sales': ['Corporate'],\n",
    " 'Signal': ['Stock'],\n",
    " 'Strategy': ['Corporate'],\n",
    " 'Technical Analysis': ['Stock', 'Corporate'],\n",
    " 'Trade': ['Economy'],\n",
    " 'Volatility': ['Market']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conditions',\n",
       " 'coverage',\n",
       " 'dividend policy',\n",
       " 'financial',\n",
       " 'fundamentals',\n",
       " 'insider activity',\n",
       " 'legal',\n",
       " 'm&a',\n",
       " 'market',\n",
       " 'options',\n",
       " 'price action',\n",
       " 'regulatory',\n",
       " 'reputation',\n",
       " 'risks',\n",
       " 'rumors',\n",
       " 'sales',\n",
       " 'signal',\n",
       " 'strategy',\n",
       " 'technical analysis',\n",
       " 'volatility'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1d9948bf4bac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maspect_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ms_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms_sn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms_score\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnippets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ms_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'snippet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ms_sn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentiment_scores'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "aspect_pairs = {}\n",
    "\n",
    "for s_id,s_sn,s_score in zip(sentences,snippets,pred_val):\n",
    "    result['results'].append({'id':s_id,'snippet':s_sn,'sentiment_scores':str(s_score)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aspect_level1= []\n",
    "aspect_level2=[]\n",
    "for asp in train_aspects[:3]:\n",
    "    print(str(asp.split('/')[1:]).replace('[', '').replace(']', '').replace('\\'', '').replace('\"',''))\n",
    "    try:\n",
    "#         print(asp)\n",
    "        aspect_level1.append(asp.split('/')[0])\n",
    "        aspect_level2.append(asp.split('/')[1:])\n",
    "        \n",
    "    except:\n",
    "        print(asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"appointment']\"], [\"risks']\"], ['sales', \"failed contract discussion']\"]]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pakhi/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pakhi/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pakhi/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pakhi/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pakhi/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pakhi/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/pakhi/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pakhi/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pakhi/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pakhi/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pakhi/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pakhi/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pprint\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import backend as K\n",
    "from bert.tokenization import FullTokenizer\n",
    "import os \n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sentence,aspect = extract_data_from_dict(master_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=30\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding dummy - doesnt pad\n",
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "      \n",
    " \n",
    " # created bert tokenizer\n",
    "def create_tokenizer_from_hub_module():\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "\n",
    "# creates the formaat for input for BERT\n",
    "class InputExample(object):\n",
    "    def __init__(self, guid, text, labels):\n",
    "        self.guid = guid\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        \n",
    "class InputFeatures(object):\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids, is_real_example=True):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids,\n",
    "        self.is_real_example=is_real_example  \n",
    "        \n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens = tokenizer.tokenize(example.text)\n",
    "    if len(tokens) > max_seq_length - 2:\n",
    "        tokens = tokens[0 : (max_seq_length - 2)]\n",
    "    \n",
    "    all_phrases = []\n",
    "    all_tokens = []\n",
    "    segment_ids = []\n",
    "    all_tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens:\n",
    "        all_tokens.append(token)\n",
    "        segment_ids.append(1)\n",
    "    all_tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "\n",
    "    all_phrases.append(example.text)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(all_tokens)\n",
    "    \n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.labels, all_phrases\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels, all_phrases = [], [], [], [], []\n",
    "    for i, example in enumerate(examples):\n",
    "        input_id, input_mask, segment_id, label, phrases = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "        all_phrases.append(phrases)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels),\n",
    "        np.array(all_phrases)\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    \n",
    "    for text, label in zip(texts, labels):\n",
    "#         print(text, label)\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text=\" \".join(text), labels=label)\n",
    "        )\n",
    "#         print(InputExamples)\n",
    "    return InputExamples\n",
    "\n",
    "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "    all_label_ids = []\n",
    "\n",
    "    for feature in features:\n",
    "        all_input_ids.append(feature.input_ids)\n",
    "        all_input_mask.append(feature.input_mask)\n",
    "        all_segment_ids.append(feature.segment_ids)\n",
    "        all_label_ids.append(feature.label_ids)\n",
    "\n",
    "    def input_fn(params):\n",
    "        batch_size = params[\"batch_size\"]\n",
    "        num_examples = len(features)\n",
    "\n",
    "        # This is for demo purposes and does NOT scale to large data sets. We do\n",
    "        # not use Dataset.from_generator() because that uses tf.py_func which is\n",
    "        # not TPU compatible. The right way to load data is with TFRecordReader.\n",
    "        \n",
    "        d = tf.data.Dataset.from_tensor_slices({\n",
    "            \"input_ids\":\n",
    "                tf.constant(\n",
    "                    all_input_ids, shape=[num_examples, seq_length],\n",
    "                    dtype=tf.int32),\n",
    "            \"input_mask\":\n",
    "                tf.constant(\n",
    "                    all_input_mask,\n",
    "                    shape=[num_examples, seq_length],\n",
    "                    dtype=tf.int32),\n",
    "            \"segment_ids\":\n",
    "                tf.constant(\n",
    "                    all_segment_ids,\n",
    "                    shape=[num_examples, seq_length],\n",
    "                    dtype=tf.int32),\n",
    "            \"label_ids\":\n",
    "                tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32)\n",
    "        })\n",
    "\n",
    "        if is_training:\n",
    "            d = d.repeat()\n",
    "            d = d.shuffle(buffer_size=100)\n",
    "\n",
    "        d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "        return d\n",
    "\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0921 06:33:27.335081 140107024676672 deprecation_wrapper.py:119] From /home/pakhi/.local/lib/python3.6/site-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer_from_hub_module() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_sentences, X, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = convert_text_to_examples(X_train, y_train)\n",
    "test_examples = convert_text_to_examples(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels, train_examples) = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels, test_examples) = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "                    def __init__(self, n_fine_tune_layers, **kwargs):\n",
    "                        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "                        self.trainable = True\n",
    "                        self.output_size = 768\n",
    "                        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "                    def build(self, input_shape):\n",
    "                        self.bert = hub.Module(\n",
    "                        bert_path,\n",
    "                        trainable=True,# did this in place of self.trainable\n",
    "                        name=\"{}_module\".format(self.name)\n",
    "                      )\n",
    "\n",
    "                        trainable_vars = self.bert.variables\n",
    "\n",
    "\n",
    "                        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "                        #print(\"--------------------------len=\",len(trainable_vars))\n",
    "                        # Select how many layers to fine tune\n",
    "                        trainable_vars = trainable_vars[-self.n_fine_tune_layers:]\n",
    "\n",
    "                        # Add to trainable weights\n",
    "                        for var in trainable_vars:\n",
    "                            self._trainable_weights.append(var)\n",
    "\n",
    "                        for var in self.bert.variables:\n",
    "                            if var not in self._trainable_weights:\n",
    "                                self._non_trainable_weights.append(var)\n",
    "\n",
    "                        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "                    def call(self, inputs):\n",
    "                        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "                        input_ids, input_mask, segment_ids = inputs\n",
    "                        bert_inputs = dict(\n",
    "                          input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "                      )\n",
    "                        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                          \"pooled_output\"\n",
    "                      ]\n",
    "                        return result\n",
    "\n",
    "                    def compute_output_shape(self, input_shape):\n",
    "                        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 506 samples, validate on 169 samples\n",
      "Epoch 1/50\n",
      "506/506 [==============================] - 27s 54ms/sample - loss: 584.1139 - f1: 0.0835 - acc: 0.0791 - val_loss: 262554.9702 - val_f1: 0.1603 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "506/506 [==============================] - 18s 35ms/sample - loss: nan - f1: nan - acc: 0.3063 - val_loss: nan - val_f1: nan - val_acc: 0.7041\n",
      "Epoch 3/50\n",
      "506/506 [==============================] - 17s 34ms/sample - loss: nan - f1: nan - acc: 0.7292 - val_loss: nan - val_f1: nan - val_acc: 0.7041\n",
      "Epoch 4/50\n",
      "506/506 [==============================] - 17s 34ms/sample - loss: nan - f1: nan - acc: 0.7292 - val_loss: nan - val_f1: nan - val_acc: 0.7041\n",
      "Epoch 5/50\n",
      "506/506 [==============================] - 18s 35ms/sample - loss: nan - f1: nan - acc: 0.7292 - val_loss: nan - val_f1: nan - val_acc: 0.7041\n",
      "Epoch 6/50\n",
      "506/506 [==============================] - 18s 35ms/sample - loss: nan - f1: nan - acc: 0.7292 - val_loss: nan - val_f1: nan - val_acc: 0.7041\n",
      "Epoch 7/50\n",
      "506/506 [==============================] - 18s 36ms/sample - loss: nan - f1: nan - acc: 0.7292 - val_loss: nan - val_f1: nan - val_acc: 0.7041\n",
      "Epoch 8/50\n",
      "256/506 [==============>...............] - ETA: 7s - loss: nan - f1: nan - acc: 0.7266"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-158609c36957>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                         validation_data=([test_input_ids, test_input_masks, test_segment_ids],test_labels))\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lr in [3e-4]:\n",
    "    for epochs in [50]:\n",
    "        for dropout in [0.5]:\n",
    "            for layers in [2]: \n",
    "                sess=tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "                from tensorflow.keras.layers import Input,Dense\n",
    "                #in_id=Input(shape=(max_seq_length,),)\n",
    "                #in_mask=Input(shape=(max_seq_length,),)\n",
    "                #in_segment=Input(shape=(max_seq_length,),)\n",
    "                in_id = tf.keras.layers.Input(shape=(max_seq_length,))\n",
    "                in_mask = tf.keras.layers.Input(shape=(max_seq_length,))\n",
    "                in_segment = tf.keras.layers.Input(shape=(max_seq_length,))\n",
    "                bert_inputs=[in_id,in_mask,in_segment]\n",
    "                bert_outputs=BertLayer(n_fine_tune_layers=6)(bert_inputs)\n",
    "                step=bert_outputs\n",
    "                if layers>=3:\n",
    "                    step=tf.keras.layers.Dense(512,activation='relu')(step)\n",
    "                    if dropout!=0:\n",
    "                        step=tf.keras.layers.Dropout(rate=dropout)(step)\n",
    "                if layers>=2:\n",
    "                    step=tf.keras.layers.Dense(256,activation='relu')(step)\n",
    "                    if dropout!=0:\n",
    "                        step=tf.keras.layers.Dropout(rate=dropout)(step)\n",
    "                if layers>=1:    \n",
    "                    step=tf.keras.layers.Dense(64,activation='relu')(step)\n",
    "                    if dropout!=0:\n",
    "                        step=tf.keras.layers.Dropout(rate=dropout)(step)\n",
    "                pred=tf.keras.layers.Dense(22,activation='softmax')(step)\n",
    "                \n",
    "#                 pred=tf.keras.layers.Dense(512,activation='relu')(step)\n",
    "                model=tf.keras.Model(inputs=bert_inputs,outputs=pred)\n",
    "                model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=tf.keras.optimizers.SGD(),\n",
    "                        metrics=[f1,'accuracy'])\n",
    "                sess.run(tf.local_variables_initializer())\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                sess.run(tf.tables_initializer())\n",
    "                K.set_session(sess)\n",
    "\n",
    "\n",
    "                model.fit([train_input_ids, train_input_masks, train_segment_ids],\n",
    "                        train_labels,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=64,\n",
    "                        validation_data=([test_input_ids, test_input_masks, test_segment_ids],test_labels))\n",
    "                print(lr,\" \",epochs,\" \",dropout,\" \" ,layers)\n",
    "                \n",
    "#                 from sklearn.metrics import f1_score,accuracy_score\n",
    "#                 predict=model.predict([test_input_ids, test_input_masks, test_segment_ids])>0.5\n",
    "#                 print(\"task=\",f1_score(test_labels,predict),\"  acc=\",accuracy_score(test_labels,predict))\n",
    "                \n",
    "                \n",
    "                model.save('model-{}-{}-{}-{}.h5'.format(lr,epochs,dropout,layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
